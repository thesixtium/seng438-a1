>   **SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 4      |
|-----------------|
| Adol Awan                |   
| Lex Berezowski              |   
| Uchenna Osemeka               |   
| Sam Shojaei                |   
| Jane Zhang               |


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction	1](#_Toc439194677)

[2 High-level description of the exploratory testing plan	1](#_Toc439194678)

[3 Comparison of exploratory and manual functional testing	1](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports	1](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was
divided	1](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned	1](#_Toc439194682)

[7 Comments/feedback on the lab and lab document itself	1](#_Toc439194683)

# Introduction

This lab concerns performing manual, and exploratory testing on version 1.0 of an ATM Simulation system developed by the Gordon College. We then perform regression testing on version 1.1 of the same system. We write detailed bug reports on bugs we find and report bugs using our chosen project management tool Atlassian's Jira.

Through this lab, we learn the basics of different approaches to testing; namely exploratory testing, and manual testing, then we practice regression testing. Prior to this lab, exploratory, manual, and regression testing were largely unknown to me. I learned the concepts of exploratory testing in the helpful article Exploratory Testing Explained posted on D2L, while manual and regression testing where learned through the lab document.


# High-level description of the exploratory testing plan

Exploratory testing is a type of testing where testers learn about the application, design tests, and excute them at the same time without relying on predefined test cases. For the ATM simulation system, testers will read Appendix B: High Level Requirements and attempt to recreate the functionality described in those requirements. This will ensure all high-level requirements are tested while maintaining the flexibility and discovery-oriented nature of exploratory testing, without requiring predefined test cases.

Exploratory testing will be conducted for 30 minutes per testing pair. Given this time constraint, not all functionality may be able to be thoroughly explored. Therefore, testers will need to strategically prioritize which functionality they deem most important and test those areas first, ensuring that critical system functions receive adequate attention.

To accomplish effective exploratory testing within the time constraint, testers will follow this structured approach:
1. Read the High Level Requirements: Testers will begin by reading Appendix B to understand the system's expected behavior. This step provides clear goals for what the system should be able to do, understanding of the complete transaction workflow, and awareness of system constraints.
2. Identify the Most Important Requirements: Based on the requirements review, testers will prioritize functionality for testing. This involves assessing which functions are used most frequently by customers, which failures would have the most severe impact on users or security,, and which areas are the most complex and therefore most prone to defects.
3. Perform Actions: For each function selected as high priority from the requirements, testers will systematically attempt to perform the action on the simulation. This involves executing each prioritized function and verifing that the system behaves as specified in the requirements.
4. Report Bugs: If the expected outcomes based on requirements and actual outcomes of an action differ, the tester will document the bug on Jira, complete with the expected outcome, actual outcome, steps taken to create the error, etc.

# Comparison of exploratory and manual functional testing

Between exploratory testing (hereafter referred to as ET) and manual functional testing (hereafter referred to as MFT), ET required more time to complete. The main reason for the lengthy process is that we needed to understand the requirements before trying to find ways to break the system. Essentially, without reading the system documentation, as developers, we have limited knowledge of how the system is meant to operate. On the other hand, the benefit is that testers have the freedom to test anything that comes to mind. However, the flexibility itself brings another tradeoff: having a smaller test suite and getting less done within the specified amount of time. The tester needs to have contextual knowledge of the various functions in order to find defects. For example, in this assignment, it was required to test an imaginary ATM application. Since we are using black box testing, the output expectations were based on the ATM requirements. If a customer is given the ability to transfer money between accounts, based on our knowledge of how ATMs work, it's expected that the correct amount should be transferred and displayed in the bank logs and the customer's receipt. If the system fails to do so, then it is recorded in the bug report. If not explicitly mentioned, we can only assume that some error message should be displayed to the customer if the transaction fails.

From the above, there is only so much that can be tested when on the go. In this case, MFT is more time-efficient because you can follow the guidelines provided. That is, for this assignment, because we did not have to write the list of test cases for scripted testing, it took significantly less time. Otherwise, the time it would've taken to plan out the test suite would be a factor of influence. As testers, we also did not need knowledge of how the system works. Simply follow the test suite document and check the function's actual outcome against the expectation. If it isn't correct, write it down. However, a tradeoff due to MFT being more structural is that some bugs may be missed, and the same ones will always be missed. Testers may miss certain bugs, big or small, that can negatively impact the user's experience using the application. 

A key takeaway from this assessment is that non-scripted and scripted test approaches should be used during software development. It very much depends on time, size, and budget constraints, but it is worthwhile to have more coverage. 

# Notes and discussion of the peer reviews of defect reports

Text…

# How the pair testing was managed and team work/effort was divided 

We divided into two subgroups: one with 2 members and one with 3 members. Each subgroup conducted approximately one hour of independent exploratory testing, followed by manual testing using the Appendix C test suite. For regression testing, we evenly distributed the defects found in ATM version 1.0 among all group members to retest on version 1.1. The manual testing for version 1.1 was also divided equally.

# Difficulties encountered, challenges overcome, and lessons learned

To discuss difficulties, we must first begin with the biggest one we faced; incorporating teamwork into the lab. Testing and script writing in general tend to be solitary in my experience, and for this lab we were all eager to actually get hands-on practice with testing. We had to decide if the suggested method of pair testing would work with what we hoped to get out of the lab. Ultimately, we decided to each perform exploratory testing completely independently, but split the manual and regression testing cases between us. This approach worked in that we all got the chance to practice exploratory testing while still collaborating on the more scripted parts.The consequence though, was that there was a lot of overlap in our findings from exploratory testing. We found less bugs to report than if we had split different focuses for our exploratory testing, or even just seen what others were working on. Going forward, we will keep fostering individual ability, but we will more clearly assign focuses and communicate more frequently, perhaps by doing some testing together in person.

Additionally we faced some difficulties in learning how to use Jira, our management software of choice, but that was quickly overcome through just practice.


# Comments/feedback on the lab and lab document itself

Text…
